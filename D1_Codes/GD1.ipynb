{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]}},"nbformat_minor":0,"nbformat":4,"cells":[{"cell_type":"markdown","source":["Project Group : GD1 <br>\n","Project Name : Melanoma Skin Detection using DCNN"],"metadata":{"id":"a3w1olTFdOUP"}},{"cell_type":"markdown","source":[" ### First, import all libraries that used in this project"],"metadata":{"id":"VuWHCzqNdOUT"}},{"cell_type":"markdown","source":["This code block contains necessary imports for the project, including Python libraries such as os, cv2, matplotlib, numpy, and pandas, as well as PyTorch and sklearn libraries. It also sets the random seeds for reproducibility and prints the list of directories in the input folder."],"metadata":{"id":"cBLV3vvBdOUU"}},{"cell_type":"code","source":["%matplotlib inline\n","# python libraties\n","import os, cv2,itertools\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","from tqdm import tqdm\n","from glob import glob\n","from PIL import Image\n","\n","# pytorch libraries\n","import torch\n","from torch import optim,nn\n","from torch.autograd import Variable\n","from torch.utils.data import DataLoader,Dataset\n","from torchvision import models,transforms\n","\n","# sklearn libraries\n","from sklearn.metrics import confusion_matrix\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report\n","\n","# to make the results are reproducible\n","np.random.seed(10)\n","torch.manual_seed(10)\n","torch.cuda.manual_seed(10)\n","\n","print(os.listdir(\"../input\"))"],"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-05-01T18:24:23.618171Z","iopub.execute_input":"2023-05-01T18:24:23.618479Z","iopub.status.idle":"2023-05-01T18:24:25.836936Z","shell.execute_reply.started":"2023-05-01T18:24:23.618420Z","shell.execute_reply":"2023-05-01T18:24:25.835713Z"},"trusted":true,"id":"LK2DSyYtdOUU","outputId":"f9da5ae5-5a8e-4e12-9ab7-b881ba0f312d"},"execution_count":null,"outputs":[{"name":"stdout","text":"['hmnist_8_8_RGB.csv', 'hmnist_28_28_RGB.csv', 'HAM10000_images_part_1', 'ham10000_images_part_1', 'hmnist_8_8_L.csv', 'HAM10000_images_part_2', 'ham10000_images_part_2', 'hmnist_28_28_L.csv', 'HAM10000_metadata.csv']\n","output_type":"stream"}]},{"cell_type":"markdown","source":["## Step 1. Data analysis and preprocessing"],"metadata":{"id":"dVmtdoJ3dOUW"}},{"cell_type":"markdown","source":["Get the all image data pathsï¼Œ match the row information in HAM10000_metadata.csv with its corresponding image"],"metadata":{"id":"jQiXOKwxdOUW"}},{"cell_type":"markdown","source":["This code block defines the path to the dataset directory and creates a dictionary `imageid_path_dict` where the keys are the image ids and the values are the paths to the corresponding images. The `lesion_type_dict` is another dictionary that maps the abbreviations for different types of skin lesions to their full names. The purpose of creating these dictionaries is to simplify the process of accessing and organizing the dataset during training and evaluation. The `glob` function is used to search for all `.jpg` files within subdirectories of the `data_dir`."],"metadata":{"id":"2vQqM_I1dOUX"}},{"cell_type":"code","source":["data_dir = '../input'\n","all_image_path = glob(os.path.join(data_dir, '*', '*.jpg'))\n","imageid_path_dict = {os.path.splitext(os.path.basename(x))[0]: x for x in all_image_path}\n","lesion_type_dict = {\n","    'nv': 'Melanocytic nevi',\n","    'mel': 'dermatofibroma',\n","    'bkl': 'Benign keratosis-like lesions ',\n","    'bcc': 'Basal cell carcinoma',\n","    'akiec': 'Actinic keratoses',\n","    'vasc': 'Vascular lesions',\n","    'df': 'Dermatofibroma'\n","}"],"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","execution":{"iopub.status.busy":"2023-05-01T18:24:25.841398Z","iopub.execute_input":"2023-05-01T18:24:25.841898Z","iopub.status.idle":"2023-05-01T18:24:49.083791Z","shell.execute_reply.started":"2023-05-01T18:24:25.841700Z","shell.execute_reply":"2023-05-01T18:24:49.082947Z"},"trusted":true,"id":"zYc9_QoEdOUX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This function is used to compute the mean and standard deviation on the whole dataset, will use for inputs normalization"],"metadata":{"id":"pQIkLCwqdOUY"}},{"cell_type":"markdown","source":["This function `compute_img_mean_std()` takes a list of image paths as input and computes the mean and standard deviation of each channel (RGB) of the images in the list. It does this by resizing each image to 224x224 pixels, stacking them in a numpy array, normalizing the pixel values to be between 0 and 1, and then computing the mean and standard deviation for each channel. The function then prints the mean and standard deviation for each channel and returns them as a tuple. This function can be used to normalize the images before training a neural network."],"metadata":{"id":"-BIzEYJedOUY"}},{"cell_type":"code","source":["def compute_img_mean_std(image_paths):\n","    \"\"\"Computes the mean and std of the three channels on the whole dataset.\"\"\"\n","    img_h, img_w = 224, 224\n","    imgs = [cv2.resize(cv2.imread(img_path), (img_h, img_w)) for img_path in tqdm(image_paths)]\n","    imgs = np.stack(imgs, axis=3).astype(np.float32) / 255.\n","    means, stdevs = [], []\n","    for i in range(3):\n","        pixels = imgs[:, :, i].astype(np.float32)\n","        means.append(np.mean(pixels, axis=(0,1)))\n","        stdevs.append(np.std(pixels, axis=(0,1)))\n","    means, stdevs = means[::-1], stdevs[::-1]  # convert BGR to RGB\n","    print(\"normMean = {}\".format(means))\n","    print(\"normStd = {}\".format(stdevs))\n","    return means, stdevs"],"metadata":{"execution":{"iopub.status.busy":"2023-05-01T18:24:49.086235Z","iopub.execute_input":"2023-05-01T18:24:49.086858Z","iopub.status.idle":"2023-05-01T18:24:49.093684Z","shell.execute_reply.started":"2023-05-01T18:24:49.086793Z","shell.execute_reply":"2023-05-01T18:24:49.092876Z"},"trusted":true,"id":"4z3Nx3jidOUa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Return the mean and std of RGB channels"],"metadata":{"id":"wmP5bv62dOUb"}},{"cell_type":"code","source":["#norm_mean,norm_std = compute_img_mean_std(all_image_path)"],"metadata":{"execution":{"iopub.status.busy":"2023-05-01T18:24:49.095212Z","iopub.execute_input":"2023-05-01T18:24:49.095816Z","iopub.status.idle":"2023-05-01T18:24:49.103427Z","shell.execute_reply.started":"2023-05-01T18:24:49.095475Z","shell.execute_reply":"2023-05-01T18:24:49.102622Z"},"trusted":true,"id":"fLch3QMedOUb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Add three columns to the original DataFrame, path (image path), cell_type (the whole name),cell_type_idx (the corresponding index  of cell type, as the image label )"],"metadata":{"id":"MgxNGdxCdOUb"}},{"cell_type":"markdown","source":["This code reads the metadata CSV file containing information about the images and adds three columns to it: 'path', 'cell_type', and 'cell_type_idx'. \n","\n","'path' column stores the path of the image file for each image id.\n","\n","'cell_type' column maps the abbreviation of each type of skin lesion to its full name using the 'lesion_type_dict' dictionary.\n","\n","'cell_type_idx' column stores the categorical codes for the skin lesion types obtained from the 'cell_type' column using the 'pd.Categorical' method.\n","\n","The resulting pandas DataFrame 'df_original' contains the metadata of all images in the dataset."],"metadata":{"id":"2h1LpXC5dOUb"}},{"cell_type":"code","source":["df_original = pd.read_csv(os.path.join(data_dir, 'HAM10000_metadata.csv'))\n","df_original['path'] = df_original['image_id'].map(imageid_path_dict.get)\n","df_original['cell_type'] = df_original['dx'].map(lesion_type_dict.get)\n","df_original['cell_type_idx'] = pd.Categorical(df_original['cell_type']).codes\n","df_original.head()"],"metadata":{"execution":{"iopub.status.busy":"2023-05-01T18:24:49.105055Z","iopub.execute_input":"2023-05-01T18:24:49.105497Z","iopub.status.idle":"2023-05-01T18:24:49.188752Z","shell.execute_reply.started":"2023-05-01T18:24:49.105318Z","shell.execute_reply":"2023-05-01T18:24:49.187950Z"},"trusted":true,"id":"OLJ1lYh2dOUc","outputId":"393f333e-fdd1-46c1-e569-99debacedb4d"},"execution_count":null,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"     lesion_id      ...      cell_type_idx\n0  HAM_0000118      ...                  2\n1  HAM_0000118      ...                  2\n2  HAM_0002730      ...                  2\n3  HAM_0002730      ...                  2\n4  HAM_0001466      ...                  2\n\n[5 rows x 10 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>lesion_id</th>\n      <th>image_id</th>\n      <th>dx</th>\n      <th>dx_type</th>\n      <th>age</th>\n      <th>sex</th>\n      <th>localization</th>\n      <th>path</th>\n      <th>cell_type</th>\n      <th>cell_type_idx</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>HAM_0000118</td>\n      <td>ISIC_0027419</td>\n      <td>bkl</td>\n      <td>histo</td>\n      <td>80.0</td>\n      <td>male</td>\n      <td>scalp</td>\n      <td>../input/ham10000_images_part_1/ISIC_0027419.jpg</td>\n      <td>Benign keratosis-like lesions</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>HAM_0000118</td>\n      <td>ISIC_0025030</td>\n      <td>bkl</td>\n      <td>histo</td>\n      <td>80.0</td>\n      <td>male</td>\n      <td>scalp</td>\n      <td>../input/ham10000_images_part_1/ISIC_0025030.jpg</td>\n      <td>Benign keratosis-like lesions</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>HAM_0002730</td>\n      <td>ISIC_0026769</td>\n      <td>bkl</td>\n      <td>histo</td>\n      <td>80.0</td>\n      <td>male</td>\n      <td>scalp</td>\n      <td>../input/ham10000_images_part_1/ISIC_0026769.jpg</td>\n      <td>Benign keratosis-like lesions</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>HAM_0002730</td>\n      <td>ISIC_0025661</td>\n      <td>bkl</td>\n      <td>histo</td>\n      <td>80.0</td>\n      <td>male</td>\n      <td>scalp</td>\n      <td>../input/ham10000_images_part_1/ISIC_0025661.jpg</td>\n      <td>Benign keratosis-like lesions</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>HAM_0001466</td>\n      <td>ISIC_0031633</td>\n      <td>bkl</td>\n      <td>histo</td>\n      <td>75.0</td>\n      <td>male</td>\n      <td>ear</td>\n      <td>../input/ham10000_images_part_2/ISIC_0031633.jpg</td>\n      <td>Benign keratosis-like lesions</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":["This code block is filtering out images that are associated with lesion_id's that have only one image. This is done to remove duplicates and make sure that each image is associated with a unique lesion_id. The resulting dataframe `df_undup` contains information about lesion_id's that have only one image associated with it. The `reset_index` method is used to reset the index of the dataframe after filtering out the lesion_id's with only one image associated with it."],"metadata":{"id":"lFXv_ebDdOUc"}},{"cell_type":"code","source":["# this will tell us how many images are associated with each lesion_id\n","df_undup = df_original.groupby('lesion_id').count()\n","# now we filter out lesion_id's that have only one image associated with it\n","df_undup = df_undup[df_undup['image_id'] == 1]\n","df_undup.reset_index(inplace=True)\n","df_undup.head()"],"metadata":{"execution":{"iopub.status.busy":"2023-05-01T18:24:49.189934Z","iopub.execute_input":"2023-05-01T18:24:49.190424Z","iopub.status.idle":"2023-05-01T18:24:49.231076Z","shell.execute_reply.started":"2023-05-01T18:24:49.190368Z","shell.execute_reply":"2023-05-01T18:24:49.230073Z"},"trusted":true,"id":"lHsOSkBudOUc","outputId":"4fab27e8-adc2-4dd0-9fb4-8d14fc084928"},"execution_count":null,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"     lesion_id  image_id  dx      ...        path  cell_type  cell_type_idx\n0  HAM_0000001         1   1      ...           1          1              1\n1  HAM_0000003         1   1      ...           1          1              1\n2  HAM_0000004         1   1      ...           1          1              1\n3  HAM_0000007         1   1      ...           1          1              1\n4  HAM_0000008         1   1      ...           1          1              1\n\n[5 rows x 10 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>lesion_id</th>\n      <th>image_id</th>\n      <th>dx</th>\n      <th>dx_type</th>\n      <th>age</th>\n      <th>sex</th>\n      <th>localization</th>\n      <th>path</th>\n      <th>cell_type</th>\n      <th>cell_type_idx</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>HAM_0000001</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>HAM_0000003</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>HAM_0000004</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>HAM_0000007</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>HAM_0000008</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":["This code defines a function `get_duplicates` that takes a lesion ID and checks whether it is present in the list of `lesion_id` values that have only one associated image in the dataset. If it is present, the function returns the string `'unduplicated'`, otherwise it returns `'duplicated'`. \n","\n","The function is then applied to a new column `duplicates` in the `df_original` dataframe, which contains the lesion ID, image ID, and other metadata for each image in the dataset. This allows us to determine whether each image is duplicated or not based on its lesion ID. \n","\n","Note that `df_undup` is a dataframe that contains only the `lesion_id` values that have a single associated image in the dataset. The function `get_duplicates` checks whether a given lesion ID is present in this list, and thus whether the image is duplicated or not."],"metadata":{"id":"1KdPOGeodOUd"}},{"cell_type":"code","source":["# here we identify lesion_id's that have duplicate images and those that have only one image.\n","def get_duplicates(x):\n","    unique_list = list(df_undup['lesion_id'])\n","    if x in unique_list:\n","        return 'unduplicated'\n","    else:\n","        return 'duplicated'\n","\n","# create a new colum that is a copy of the lesion_id column\n","df_original['duplicates'] = df_original['lesion_id']\n","# apply the function to this new column\n","df_original['duplicates'] = df_original['duplicates'].apply(get_duplicates)\n","df_original.head()"],"metadata":{"execution":{"iopub.status.busy":"2023-05-01T18:24:49.232548Z","iopub.execute_input":"2023-05-01T18:24:49.232977Z","iopub.status.idle":"2023-05-01T18:24:51.222631Z","shell.execute_reply.started":"2023-05-01T18:24:49.232799Z","shell.execute_reply":"2023-05-01T18:24:51.221809Z"},"trusted":true,"id":"ajzbeOPBdOUe","outputId":"159efb73-dbc5-4629-be11-e0a627eda94c"},"execution_count":null,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"     lesion_id      image_id     ...     cell_type_idx  duplicates\n0  HAM_0000118  ISIC_0027419     ...                 2  duplicated\n1  HAM_0000118  ISIC_0025030     ...                 2  duplicated\n2  HAM_0002730  ISIC_0026769     ...                 2  duplicated\n3  HAM_0002730  ISIC_0025661     ...                 2  duplicated\n4  HAM_0001466  ISIC_0031633     ...                 2  duplicated\n\n[5 rows x 11 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>lesion_id</th>\n      <th>image_id</th>\n      <th>dx</th>\n      <th>dx_type</th>\n      <th>age</th>\n      <th>sex</th>\n      <th>localization</th>\n      <th>path</th>\n      <th>cell_type</th>\n      <th>cell_type_idx</th>\n      <th>duplicates</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>HAM_0000118</td>\n      <td>ISIC_0027419</td>\n      <td>bkl</td>\n      <td>histo</td>\n      <td>80.0</td>\n      <td>male</td>\n      <td>scalp</td>\n      <td>../input/ham10000_images_part_1/ISIC_0027419.jpg</td>\n      <td>Benign keratosis-like lesions</td>\n      <td>2</td>\n      <td>duplicated</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>HAM_0000118</td>\n      <td>ISIC_0025030</td>\n      <td>bkl</td>\n      <td>histo</td>\n      <td>80.0</td>\n      <td>male</td>\n      <td>scalp</td>\n      <td>../input/ham10000_images_part_1/ISIC_0025030.jpg</td>\n      <td>Benign keratosis-like lesions</td>\n      <td>2</td>\n      <td>duplicated</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>HAM_0002730</td>\n      <td>ISIC_0026769</td>\n      <td>bkl</td>\n      <td>histo</td>\n      <td>80.0</td>\n      <td>male</td>\n      <td>scalp</td>\n      <td>../input/ham10000_images_part_1/ISIC_0026769.jpg</td>\n      <td>Benign keratosis-like lesions</td>\n      <td>2</td>\n      <td>duplicated</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>HAM_0002730</td>\n      <td>ISIC_0025661</td>\n      <td>bkl</td>\n      <td>histo</td>\n      <td>80.0</td>\n      <td>male</td>\n      <td>scalp</td>\n      <td>../input/ham10000_images_part_1/ISIC_0025661.jpg</td>\n      <td>Benign keratosis-like lesions</td>\n      <td>2</td>\n      <td>duplicated</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>HAM_0001466</td>\n      <td>ISIC_0031633</td>\n      <td>bkl</td>\n      <td>histo</td>\n      <td>75.0</td>\n      <td>male</td>\n      <td>ear</td>\n      <td>../input/ham10000_images_part_2/ISIC_0031633.jpg</td>\n      <td>Benign keratosis-like lesions</td>\n      <td>2</td>\n      <td>duplicated</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":["df_original['duplicates'].value_counts()"],"metadata":{"execution":{"iopub.status.busy":"2023-05-01T18:24:51.223929Z","iopub.execute_input":"2023-05-01T18:24:51.224408Z","iopub.status.idle":"2023-05-01T18:24:51.235486Z","shell.execute_reply.started":"2023-05-01T18:24:51.224351Z","shell.execute_reply":"2023-05-01T18:24:51.234315Z"},"trusted":true,"id":"8eMkWuZldOUe","outputId":"eefe7d22-b92a-47b4-d3d0-a121c431306e"},"execution_count":null,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"unduplicated    5514\nduplicated      4501\nName: duplicates, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":["# now we filter out images that don't have duplicates\n","df_undup = df_original[df_original['duplicates'] == 'unduplicated']\n","df_undup.shape"],"metadata":{"execution":{"iopub.status.busy":"2023-05-01T18:24:51.237685Z","iopub.execute_input":"2023-05-01T18:24:51.239360Z","iopub.status.idle":"2023-05-01T18:24:51.251069Z","shell.execute_reply.started":"2023-05-01T18:24:51.238198Z","shell.execute_reply":"2023-05-01T18:24:51.249791Z"},"trusted":true,"id":"cQA8Q8GWdOUf","outputId":"26be99e7-e5d7-435d-f1e8-83ea78efd93b"},"execution_count":null,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"(5514, 11)"},"metadata":{}}]},{"cell_type":"code","source":["# now we create a val set using df because we are sure that none of these images have augmented duplicates in the train set\n","y = df_undup['cell_type_idx']\n","_, df_val = train_test_split(df_undup, test_size=0.2, random_state=101, stratify=y)\n","df_val.shape"],"metadata":{"execution":{"iopub.status.busy":"2023-05-01T18:24:51.252510Z","iopub.execute_input":"2023-05-01T18:24:51.252939Z","iopub.status.idle":"2023-05-01T18:24:51.268151Z","shell.execute_reply.started":"2023-05-01T18:24:51.252752Z","shell.execute_reply":"2023-05-01T18:24:51.267188Z"},"trusted":true,"id":"WTQIrV2idOUf","outputId":"fc60ab6a-5f15-4e8a-d911-9a373c9bb26e"},"execution_count":null,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"(1103, 11)"},"metadata":{}}]},{"cell_type":"code","source":["df_val['cell_type_idx'].value_counts()"],"metadata":{"execution":{"iopub.status.busy":"2023-05-01T18:24:51.269694Z","iopub.execute_input":"2023-05-01T18:24:51.270202Z","iopub.status.idle":"2023-05-01T18:24:51.278651Z","shell.execute_reply.started":"2023-05-01T18:24:51.269952Z","shell.execute_reply":"2023-05-01T18:24:51.277835Z"},"trusted":true,"id":"qilwUa2ddOUg","outputId":"057c52ea-f0ad-4279-acd8-75d559b23d40"},"execution_count":null,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"4    883\n2     88\n6     46\n1     35\n0     30\n5     13\n3      8\nName: cell_type_idx, dtype: int64"},"metadata":{}}]},{"cell_type":"markdown","source":["This code defines a function `get_val_rows` that takes an image ID and returns whether it is in the validation or training set. It then applies this function to the `image_id` column of `df_original` and stores the result in a new column called `train_or_val`.\n","\n","It then filters out the rows of `df_original` that correspond to the training set by creating a new dataframe `df_train` that only includes the rows where `train_or_val` is equal to `'train'`. It also prints out the number of rows in `df_train` and `df_val`."],"metadata":{"id":"Kplpfok8dOUg"}},{"cell_type":"code","source":["# This set will be df_original excluding all rows that are in the val set\n","# This function identifies if an image is part of the train or val set.\n","def get_val_rows(x):\n","    # create a list of all the lesion_id's in the val set\n","    val_list = list(df_val['image_id'])\n","    if str(x) in val_list:\n","        return 'val'\n","    else:\n","        return 'train'\n","\n","# identify train and val rows\n","# create a new colum that is a copy of the image_id column\n","df_original['train_or_val'] = df_original['image_id']\n","# apply the function to this new column\n","df_original['train_or_val'] = df_original['train_or_val'].apply(get_val_rows)\n","# filter out train rows\n","df_train = df_original[df_original['train_or_val'] == 'train']\n","print(len(df_train))\n","print(len(df_val))"],"metadata":{"execution":{"iopub.status.busy":"2023-05-01T18:24:51.280133Z","iopub.execute_input":"2023-05-01T18:24:51.280759Z","iopub.status.idle":"2023-05-01T18:24:51.949776Z","shell.execute_reply.started":"2023-05-01T18:24:51.280676Z","shell.execute_reply":"2023-05-01T18:24:51.948969Z"},"trusted":true,"id":"vwdtRLEKdOUg","outputId":"1cb961ba-42c3-49a7-e58e-89806b30d2e4"},"execution_count":null,"outputs":[{"name":"stdout","text":"8912\n1103\n","output_type":"stream"}]},{"cell_type":"code","source":["df_train['cell_type_idx'].value_counts()"],"metadata":{"execution":{"iopub.status.busy":"2023-05-01T18:24:51.951144Z","iopub.execute_input":"2023-05-01T18:24:51.951711Z","iopub.status.idle":"2023-05-01T18:24:51.960082Z","shell.execute_reply.started":"2023-05-01T18:24:51.951652Z","shell.execute_reply":"2023-05-01T18:24:51.959051Z"},"trusted":true,"id":"ckrqSa5DdOUh","outputId":"92dcee91-b357-4ff6-96b5-bd4ee3d17cba"},"execution_count":null,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"4    5822\n6    1067\n2    1011\n1     479\n0     297\n5     129\n3     107\nName: cell_type_idx, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":["df_val['cell_type'].value_counts()"],"metadata":{"execution":{"iopub.status.busy":"2023-05-01T18:24:51.961711Z","iopub.execute_input":"2023-05-01T18:24:51.962307Z","iopub.status.idle":"2023-05-01T18:24:51.973258Z","shell.execute_reply.started":"2023-05-01T18:24:51.961972Z","shell.execute_reply":"2023-05-01T18:24:51.972155Z"},"trusted":true,"id":"fICNelXhdOUh","outputId":"10383e67-7130-4590-cc53-a65f37b7b2e7"},"execution_count":null,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"Melanocytic nevi                  883\nBenign keratosis-like lesions      88\ndermatofibroma                     46\nBasal cell carcinoma               35\nActinic keratoses                  30\nVascular lesions                   13\nDermatofibroma                      8\nName: cell_type, dtype: int64"},"metadata":{}}]},{"cell_type":"markdown","source":["**From From the above statistics of each category, we can see that there is a serious class imbalance in the training data. To solve this problem, I think we can start from two aspects, one is equalization sampling, and the other is a loss function that can be used to mitigate category imbalance during training, such as focal loss.**"],"metadata":{"id":"mm1cbICFdOUh"}},{"cell_type":"markdown","source":["This code snippet is performing data augmentation by creating more samples for some of the classes that have fewer samples compared to the other classes. It does this by copying some of the existing samples in the training set multiple times. The number of copies to be made for each class is specified by the list `data_aug_rate`, which has 7 elements (one for each class) that represent the number of copies to be made for each class. \n","\n","For example, `data_aug_rate = [15,10,5,50,0,40,5]` means that for the first class (Melanocytic nevi), 15 additional copies will be made, for the second class (dermatofibroma), 10 additional copies will be made, for the third class (Benign keratosis-like lesions), 5 additional copies will be made, and so on. \n","\n","The `append` method is used to append the selected samples to the existing training set for each class. The `ignore_index=True` argument ensures that the index values of the appended rows are reset to avoid duplicate index values. \n","\n","Finally, the `value_counts` method is used to verify that the number of samples for each class is now balanced."],"metadata":{"id":"iArHf02MdOUh"}},{"cell_type":"code","source":["# Copy fewer class to balance the number of 7 classes\n","data_aug_rate = [15,10,5,50,0,40,5]\n","for i in range(7):\n","    if data_aug_rate[i]:\n","        df_train=df_train.append([df_train.loc[df_train['cell_type_idx'] == i,:]]*(data_aug_rate[i]-1), ignore_index=True)\n","df_train['cell_type'].value_counts()"],"metadata":{"execution":{"iopub.status.busy":"2023-05-01T18:24:51.974816Z","iopub.execute_input":"2023-05-01T18:24:51.975459Z","iopub.status.idle":"2023-05-01T18:24:52.117042Z","shell.execute_reply.started":"2023-05-01T18:24:51.975153Z","shell.execute_reply":"2023-05-01T18:24:52.116092Z"},"trusted":true,"id":"ILtqxchxdOUh","outputId":"b715b833-9b47-4985-b6d3-a03657e95094"},"execution_count":null,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"Melanocytic nevi                  5822\nDermatofibroma                    5350\ndermatofibroma                    5335\nVascular lesions                  5160\nBenign keratosis-like lesions     5055\nBasal cell carcinoma              4790\nActinic keratoses                 4455\nName: cell_type, dtype: int64"},"metadata":{}}]},{"cell_type":"markdown","source":["At the beginning, I divided the data into three parts, training set, validation set and test set. Considering the small amount of data, I did not further divide the validation set data in practice."],"metadata":{"id":"PvDSmj9tdOUh"}},{"cell_type":"code","source":["# # We can split the test set again in a validation set and a true test set:\n","# df_val, df_test = train_test_split(df_val, test_size=0.5)\n","df_train = df_train.reset_index()\n","df_val = df_val.reset_index()\n","# df_test = df_test.reset_index()"],"metadata":{"execution":{"iopub.status.busy":"2023-05-01T18:24:52.118735Z","iopub.execute_input":"2023-05-01T18:24:52.119183Z","iopub.status.idle":"2023-05-01T18:24:52.144173Z","shell.execute_reply.started":"2023-05-01T18:24:52.118987Z","shell.execute_reply":"2023-05-01T18:24:52.143545Z"},"trusted":true,"id":"d-dwcCdndOUi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Step 2. Model building"],"metadata":{"id":"UvM4TB7TdOUi"}},{"cell_type":"markdown","source":["This function takes a PyTorch model and a boolean value indicating whether to perform feature extraction or fine-tuning. If `feature_extracting` is set to `True`, all parameters in the model are set to not require gradients (`param.requires_grad = False`) so that only the newly added layers require gradients during training. If `feature_extracting` is set to `False`, all parameters in the model require gradients and can be fine-tuned during training."],"metadata":{"id":"zZWQrUf7dOUi"}},{"cell_type":"code","source":["# feature_extract is a boolean that defines if we are finetuning or feature extracting. \n","# If feature_extract = False, the model is finetuned and all model parameters are updated. \n","# If feature_extract = True, only the last layer parameters are updated, the others remain fixed.\n","def set_parameter_requires_grad(model, feature_extracting):\n","    if feature_extracting:\n","        for param in model.parameters():\n","            param.requires_grad = False"],"metadata":{"execution":{"iopub.status.busy":"2023-05-01T18:24:52.145711Z","iopub.execute_input":"2023-05-01T18:24:52.146242Z","iopub.status.idle":"2023-05-01T18:24:52.150651Z","shell.execute_reply.started":"2023-05-01T18:24:52.146109Z","shell.execute_reply":"2023-05-01T18:24:52.149767Z"},"trusted":true,"id":"d0kma03ldOUi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This function initializes a CNN model for transfer learning using a pre-trained model. The function takes four arguments:\n","\n","- `model_name`: A string that specifies which pre-trained model to use. Currently, only the \"densenet\" model is implemented.\n","- `num_classes`: An integer that specifies the number of output classes of the final classification layer.\n","- `feature_extract`: A boolean that specifies whether to fine-tune the entire model or only the final layer.\n","- `use_pretrained`: A boolean that specifies whether to use the pre-trained weights for the model.\n","\n","The function initializes the specified pre-trained model, sets the `requires_grad` parameter of the layers to be fine-tuned according to the `feature_extract` argument, replaces the final classification layer with a new fully connected layer with the number of output classes specified by the `num_classes` argument, and returns the modified model and the input size of the model (which is fixed at 224 for all pre-trained models currently implemented)."],"metadata":{"id":"66_V3yiYdOUi"}},{"cell_type":"code","source":["def initialize_model(model_name, num_classes, feature_extract, use_pretrained=True):\n","    # Initialize these variables which will be set in this if statement. Each of these\n","    #   variables is model specific.\n","    model_ft = None\n","    input_size = 0\n","\n","    if model_name == model_name == \"densenet\":\n","        \"\"\" Densenet121\n","        \"\"\"\n","        model_ft = models.densenet121(pretrained=use_pretrained)\n","        '''model_ft.fc = nn.Sequential(\n","        nn.Linear(num_classes, 1024),\n","        nn.BatchNorm1d(1024),\n","        nn.ReLU(inplace=True),\n","        nn.Linear(1024, 512),\n","        nn.BatchNorm1d(512),\n","        nn.ReLU(inplace=True),\n","        nn.Linear(512, 256),\n","        nn.BatchNorm1d(256),\n","        nn.ReLU(inplace=True),\n","        nn.Linear(256, 128),\n","        nn.BatchNorm1d(128),\n","        nn.ReLU(inplace=True),\n","        nn.Linear(128, num_classes))'''\n","        set_parameter_requires_grad(model_ft, feature_extract)\n","        num_ftrs = model_ft.classifier.in_features\n","        model_ft.classifier = nn.Linear(num_ftrs, num_classes)\n","        input_size = 224\n","\n","    elif model_name == \"inception\":\n","        \"\"\" Inception v3\n","        Be careful, expects (299,299) sized images and has auxiliary output\n","        \"\"\"\n","        model_ft = models.inception_v3(pretrained=use_pretrained)\n","        set_parameter_requires_grad(model_ft, feature_extract)\n","        # Handle the auxilary net\n","        num_ftrs = model_ft.AuxLogits.fc.in_features\n","        model_ft.AuxLogits.fc = nn.Linear(num_ftrs, num_classes)\n","        # Handle the primary net\n","        num_ftrs = model_ft.fc.in_features\n","        model_ft.fc = nn.Linear(num_ftrs,num_classes)\n","        input_size = 299\n","\n","    else:\n","        print(\"Invalid model name, exiting...\")\n","        exit()\n","    return model_ft, input_size"],"metadata":{"execution":{"iopub.status.busy":"2023-05-01T18:24:52.152369Z","iopub.execute_input":"2023-05-01T18:24:52.152821Z","iopub.status.idle":"2023-05-01T18:24:52.163425Z","shell.execute_reply.started":"2023-05-01T18:24:52.152670Z","shell.execute_reply":"2023-05-01T18:24:52.162443Z"},"trusted":true,"id":"y_0rbljDdOUi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["You can change your backbone network, here are 4 different networks, each network also has sevaral versions. Considering the limited training data, we used the ImageNet pre-training model for fine-tuning. This can speed up the convergence of the model and improve the accuracy.\n","\n","There is one thing you need to pay attention to, the input size of Inception is different from the others (299x299), you need to change the setting of compute_img_mean_std() function "],"metadata":{"id":"vN8lNDkNdOUj"}},{"cell_type":"markdown","source":["Sure, here is the documentation for the code above:\n","\n","- `model_name`: a string variable that specifies the name of the model to be used for transfer learning. It can be 'resnet', 'vgg', 'densenet', or 'inception'.\n","- `num_classes`: an integer variable that specifies the number of classes in the dataset.\n","- `feature_extract`: a boolean variable that specifies whether to finetune all layers of the model or only the top layer.\n","- `model_ft`: a variable that holds the pretrained model.\n","- `input_size`: an integer variable that specifies the input size of the model.\n","- `initialize_model`: a function that initializes the specified model for transfer learning by changing its classifier layer to output the desired number of classes.\n","- `device`: a variable that specifies the device (CPU or GPU) to use for training the model.\n","- `model`: a variable that holds the model to be used for training. It is moved to the specified device."],"metadata":{"id":"tM--4yW-dOUr"}},{"cell_type":"code","source":["# resnet,vgg,densenet,inception\n","model_name = 'inception'\n","num_classes = 7\n","feature_extract = False\n","# Initialize the model for this run\n","model_ft, input_size = initialize_model(model_name, num_classes, feature_extract, use_pretrained=True)\n","# Define the device:\n","device = torch.device('cuda:0')\n","# Put the model on the device:\n","model = model_ft.to(device)"],"metadata":{"execution":{"iopub.status.busy":"2023-05-01T18:24:52.164674Z","iopub.execute_input":"2023-05-01T18:24:52.165398Z","iopub.status.idle":"2023-05-01T18:24:59.956969Z","shell.execute_reply.started":"2023-05-01T18:24:52.164999Z","shell.execute_reply":"2023-05-01T18:24:59.956225Z"},"trusted":true,"id":"5VubTNXQdOUr","outputId":"69b382cc-742b-4b5e-dbcc-a293450cdb0c"},"execution_count":null,"outputs":[{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/inception_v3_google-1a9a5a14.pth\" to /root/.torch/models/inception_v3_google-1a9a5a14.pth\n108857766it [00:00, 111340717.65it/s]\n","output_type":"stream"}]},{"cell_type":"markdown","source":["Here are the documentations for the code above:\n","\n","- `norm_mean` and `norm_std` are the mean and standard deviation values for each color channel (RGB) of the input images. These values are used to normalize the input images during the training and validation process.\n","\n","- `train_transform` is a composition of several image transformation techniques that will be applied to the training set of images. These techniques include resizing the image to `input_size`, random horizontal and vertical flips, random rotations up to 20 degrees, color jittering, and normalization using the `norm_mean` and `norm_std` values.\n","\n","- `val_transform` is similar to `train_transform`, but it only applies resizing and normalization to the validation set of images."],"metadata":{"id":"5y30Do_qdOUr"}},{"cell_type":"code","source":["norm_mean = (0.49139968, 0.48215827, 0.44653124)\n","norm_std = (0.24703233, 0.24348505, 0.26158768)\n","# define the transformation of the train images.\n","train_transform = transforms.Compose([transforms.Resize((input_size,input_size)),transforms.RandomHorizontalFlip(),\n","                                      transforms.RandomVerticalFlip(),transforms.RandomRotation(20),\n","                                      transforms.ColorJitter(brightness=0.1, contrast=0.1, hue=0.1),\n","                                        transforms.ToTensor(), transforms.Normalize(norm_mean, norm_std)])\n","# define the transformation of the val images.\n","val_transform = transforms.Compose([transforms.Resize((input_size,input_size)), transforms.ToTensor(),\n","                                    transforms.Normalize(norm_mean, norm_std)])"],"metadata":{"execution":{"iopub.status.busy":"2023-05-01T18:24:59.958254Z","iopub.execute_input":"2023-05-01T18:24:59.958553Z","iopub.status.idle":"2023-05-01T18:24:59.967519Z","shell.execute_reply.started":"2023-05-01T18:24:59.958505Z","shell.execute_reply":"2023-05-01T18:24:59.966804Z"},"trusted":true,"id":"DeKQuzawdOUr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This is a custom PyTorch dataset class called HAM10000 that is defined to load the data for the HAM10000 dataset. It takes a dataframe and an optional transformation as input. \n","\n","- `__init__(self, df, transform=None)` : Initializes the class with the dataframe and the transformation.\n","\n","- `__len__(self)` : Returns the length of the dataset.\n","\n","- `__getitem__(self, index)` : Returns a tuple of the image data and its corresponding label. It takes an index as input and returns the transformed image and its corresponding label as a PyTorch tensor."],"metadata":{"id":"2SvmFcGxdOUs"}},{"cell_type":"code","source":["# Define a pytorch dataloader for this dataset\n","class HAM10000(Dataset):\n","    def __init__(self, df, transform=None):\n","        self.df = df\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def __getitem__(self, index):\n","        # Load data and get label\n","        X = Image.open(self.df['path'][index])\n","        y = torch.tensor(int(self.df['cell_type_idx'][index]))\n","\n","        if self.transform:\n","            X = self.transform(X)\n","\n","        return X, y"],"metadata":{"execution":{"iopub.status.busy":"2023-05-01T18:24:59.971047Z","iopub.execute_input":"2023-05-01T18:24:59.971319Z","iopub.status.idle":"2023-05-01T18:24:59.979495Z","shell.execute_reply.started":"2023-05-01T18:24:59.971256Z","shell.execute_reply":"2023-05-01T18:24:59.978639Z"},"trusted":true,"id":"ni7C9_bOdOUs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Here we define the training and validation sets using the table train_df and our defined transformations (train_transform and val_transform). We use the PyTorch DataLoader to create batches of images to feed to the model during training and validation. We set the batch size to 32, shuffle the training set, and do not shuffle the validation set. We also set the number of workers to 4 to parallelize the data loading process."],"metadata":{"id":"2K5urzfhdOUs"}},{"cell_type":"code","source":["# Define the training set using the table train_df and using our defined transitions (train_transform)\n","training_set = HAM10000(df_train, transform=train_transform)\n","train_loader = DataLoader(training_set, batch_size=32, shuffle=True, num_workers=4)\n","# Same for the validation set:\n","validation_set = HAM10000(df_val, transform=train_transform)\n","val_loader = DataLoader(validation_set, batch_size=32, shuffle=False, num_workers=4)"],"metadata":{"execution":{"iopub.status.busy":"2023-05-01T18:24:59.981186Z","iopub.execute_input":"2023-05-01T18:24:59.981677Z","iopub.status.idle":"2023-05-01T18:24:59.989407Z","shell.execute_reply.started":"2023-05-01T18:24:59.981484Z","shell.execute_reply":"2023-05-01T18:24:59.988411Z"},"trusted":true,"id":"AS_o-IuEdOUs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This code initializes an Adam optimizer with a learning rate of 0.001 and sets the cross entropy loss as the loss function. The model.parameters() method is used to retrieve the parameters of the model that need to be optimized. The optimizer will be used to update these parameters during training to minimize the loss function. The to(device) method is used to move the loss function to the same device as the model, which is typically a GPU."],"metadata":{"id":"ZdjETJWmdOUs"}},{"cell_type":"code","source":["# we use Adam optimizer, use cross entropy loss as our loss function\n","optimizer = optim.Adam(model.parameters(), lr=1e-3)\n","criterion = nn.CrossEntropyLoss().to(device)"],"metadata":{"execution":{"iopub.status.busy":"2023-05-01T18:24:59.990816Z","iopub.execute_input":"2023-05-01T18:24:59.991373Z","iopub.status.idle":"2023-05-01T18:25:00.001996Z","shell.execute_reply.started":"2023-05-01T18:24:59.991235Z","shell.execute_reply":"2023-05-01T18:25:00.001054Z"},"trusted":true,"id":"4XBs2T_rdOUs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Step 3. Model training"],"metadata":{"id":"LCRI2egndOUt"}},{"cell_type":"markdown","source":["The `AverageMeter` class is used during training process to calculate the loss and accuracy. It has four attributes: `val`, `avg`, `sum`, and `count`. `val` is the most recent value that has been calculated, `sum` is the sum of all the values calculated so far, `count` is the number of values calculated so far, and `avg` is the average of all the values calculated so far. It has three methods: `reset`, `update`, and `__init__`. The `reset` method resets all attributes to their initial values. The `update` method updates the values of `val`, `sum`, `count`, and `avg` based on a new input value `val`. The `__init__` method initializes all attributes to their initial values by calling the `reset` method."],"metadata":{"id":"wMe4SkwvdOUt"}},{"cell_type":"code","source":["# this function is used during training process, to calculation the loss and accuracy\n","class AverageMeter(object):\n","    def __init__(self):\n","        self.reset()\n","\n","    def reset(self):\n","        self.val = 0\n","        self.avg = 0\n","        self.sum = 0\n","        self.count = 0\n","\n","    def update(self, val, n=1):\n","        self.val = val\n","        self.sum += val * n\n","        self.count += n\n","        self.avg = self.sum / self.count"],"metadata":{"execution":{"iopub.status.busy":"2023-05-01T18:25:00.005325Z","iopub.execute_input":"2023-05-01T18:25:00.005567Z","iopub.status.idle":"2023-05-01T18:25:00.013473Z","shell.execute_reply.started":"2023-05-01T18:25:00.005513Z","shell.execute_reply":"2023-05-01T18:25:00.012437Z"},"trusted":true,"id":"fSnsgc9XdOUt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The `train` function is used during the training process to update the weights of the model using the optimizer and the loss function. It takes in the training data loader, the model, the loss function, the optimizer, and the current epoch number as inputs. Inside the function, for each batch of data, it loads the images and labels and performs forward propagation through the model to get the outputs. Then, it calculates the loss based on the predicted outputs and the actual labels, and backpropagates the loss through the model to calculate the gradients. Finally, it updates the model parameters using the optimizer based on the calculated gradients. The function also calculates and returns the average loss and accuracy for the current epoch."],"metadata":{"id":"_yYvisW_dOUu"}},{"cell_type":"code","source":["total_loss_train, total_acc_train = [],[]\n","def train(train_loader, model, criterion, optimizer, epoch):\n","    model.train()\n","    train_loss = AverageMeter()\n","    train_acc = AverageMeter()\n","    curr_iter = (epoch - 1) * len(train_loader)\n","    for i, data in enumerate(train_loader):\n","        images, labels = data\n","        N = images.size(0)\n","        # print('image shape:',images.size(0), 'label shape',labels.size(0))\n","        images = Variable(images).to(device)\n","        labels = Variable(labels).to(device)\n","\n","        optimizer.zero_grad()\n","        outputs = model(images)\n","\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","        prediction = outputs.max(1, keepdim=True)[1]\n","        train_acc.update(prediction.eq(labels.view_as(prediction)).sum().item()/N)\n","        train_loss.update(loss.item())\n","        curr_iter += 1\n","        if (i + 1) % 100 == 0:\n","            print(f\"[epoch {epoch}], [iter {i + 1}/{len(train_loader)}], \"f\"[train loss {train_loss.avg:.5f}], \"f\"[train acc {train_acc.avg:.5f}]\")\n","            total_loss_train.append(train_loss.avg)\n","            total_acc_train.append(train_acc.avg)\n","    return train_loss.avg, train_acc.avg"],"metadata":{"execution":{"iopub.status.busy":"2023-05-01T18:25:00.014831Z","iopub.execute_input":"2023-05-01T18:25:00.015155Z","iopub.status.idle":"2023-05-01T18:25:00.024925Z","shell.execute_reply.started":"2023-05-01T18:25:00.015081Z","shell.execute_reply":"2023-05-01T18:25:00.024028Z"},"trusted":true,"id":"EImSdtLLdOUu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The `validate` function is used to evaluate the model on the validation set after each training epoch. It takes in the validation dataloader, the model, the loss criterion, the optimizer, and the current epoch as input. \n","\n","Within the function, the model is set to evaluation mode using `model.eval()`, which turns off the gradient computation and activates the evaluation mode for any layers that have different behavior during training and evaluation. Then, we iterate through the validation dataloader and compute the validation loss and accuracy for each batch. We update the `val_loss` and `val_acc` using the `AverageMeter()` function. \n","\n","Finally, we print the validation loss and accuracy, and return these values."],"metadata":{"id":"nFWnN0SidOUu"}},{"cell_type":"code","source":["def validate(val_loader, model, criterion, optimizer, epoch):\n","    model.eval()\n","    val_loss = AverageMeter()\n","    val_acc = AverageMeter()\n","    with torch.no_grad():\n","        for i, data in enumerate(val_loader):\n","            images, labels = data\n","            N = images.size(0)\n","            images = Variable(images).to(device)\n","            labels = Variable(labels).to(device)\n","\n","            outputs = model(images)\n","            prediction = outputs.max(1, keepdim=True)[1]\n","\n","            val_acc.update(prediction.eq(labels.view_as(prediction)).sum().item()/N)\n","\n","            val_loss.update(criterion(outputs, labels).item())\n","\n","    print('------------------------------------------------------------')\n","    print('[epoch %d], [val loss %.5f], [val acc %.5f]' % (epoch, val_loss.avg, val_acc.avg))\n","    print('------------------------------------------------------------')\n","    return val_loss.avg, val_acc.avg"],"metadata":{"execution":{"iopub.status.busy":"2023-05-01T18:25:00.026544Z","iopub.execute_input":"2023-05-01T18:25:00.027128Z","iopub.status.idle":"2023-05-01T18:25:00.035861Z","shell.execute_reply.started":"2023-05-01T18:25:00.027073Z","shell.execute_reply":"2023-05-01T18:25:00.035314Z"},"trusted":true,"id":"2QG4H7nFdOUu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This code segment trains a PyTorch model on a given dataset for a specified number of epochs. \n","\n","`epoch_num` specifies the number of epochs to train for. `best_val_acc` is a variable that is used to keep track of the highest validation accuracy achieved during training. `total_loss_val` and `total_acc_val` are empty lists that will be used to store the validation loss and accuracy for each epoch. \n","\n","`model` is moved to the device (e.g. GPU) outside of the loop. This is done to prevent repeatedly moving the model to the device for each epoch, which can be time-consuming. \n","\n","The loop iterates over the number of epochs specified by `epoch_num`. For each epoch, the `train` function is called with the training data, model, criterion, optimizer, and epoch number as arguments. The `train` function returns the average training loss and accuracy for that epoch. \n","\n","After the training is complete for an epoch, the `validate` function is called with the validation data, model, criterion, optimizer, and epoch number as arguments. The `validate` function calculates the validation loss and accuracy for the current epoch. `torch.no_grad()` is used during validation to disable gradient computation, which can save time and memory. \n","\n","The validation loss and accuracy for the current epoch are then appended to `total_loss_val` and `total_acc_val` lists, respectively. If the current validation accuracy is higher than the previous best accuracy (`best_val_acc`), the `best_val_acc` variable is updated, and a message is printed indicating that a new best accuracy has been achieved. \n","\n","Finally, `torch.cuda.empty_cache()` is called to release GPU memory after each epoch, which can help prevent out-of-memory errors."],"metadata":{"id":"xn-jqBo6dOUv"}},{"cell_type":"code","source":["epoch_num = 50\n","best_val_acc = 0\n","total_loss_val, total_acc_val = [],[]\n","model = model.to(device) # move the model to the device outside the loops\n","for epoch in range(1, epoch_num+1):\n","    train_loss, train_acc = train(train_loader, model, criterion, optimizer, epoch)\n","    with torch.no_grad(): # use torch.no_grad() during validation\n","        val_loss, val_acc = validate(val_loader, model, criterion, optimizer, epoch)\n","    total_loss_val.append(val_loss)\n","    total_acc_val.append(val_acc)\n","    if val_acc > best_val_acc:\n","        best_val_acc = val_acc\n","        filename_state = f'model_{epoch}ch__ValAcc_{val_acc}_state.pth'\n","        torch.save(model.state_dict(), filename_state)\n","        filename = f'model_{epoch}ch__ValAcc_{val_acc}.pth'\n","        torch.save(model, filename)\n","        print('*****************************************************')\n","        print('best record: [epoch %d], [val loss %.5f], [val acc %.5f]' % (epoch, val_loss, val_acc))\n","        print('*****************************************************')\n","    torch.cuda.empty_cache() # release GPU memory after each epoch"],"metadata":{"execution":{"iopub.status.busy":"2023-05-01T18:25:00.037091Z","iopub.execute_input":"2023-05-01T18:25:00.037615Z","iopub.status.idle":"2023-05-01T18:25:07.028236Z","shell.execute_reply.started":"2023-05-01T18:25:00.037564Z","shell.execute_reply":"2023-05-01T18:25:06.963434Z"},"trusted":true,"id":"3q7clUbpdOUv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Step 4. Model evaluation"],"metadata":{"id":"BRg9iEHtdOUv"}},{"cell_type":"markdown","source":["This code segment generates a plot of training and validation loss/accuracy over multiple epochs using the Matplotlib library. \n","\n","- `ax.plot(total_loss_train, label='training loss')` plots the training loss on the first y-axis (`ax`). `total_loss_train` is a list of losses obtained during training. The label `training loss` is used to identify this line in the legend.\n","- `ax.plot(total_acc_train, label='training accuracy')` plots the training accuracy on the first y-axis (`ax`). `total_acc_train` is a list of accuracies obtained during training. The label `training accuracy` is used to identify this line in the legend.\n","- `ax.set_ylabel('training loss/accuracy')` sets the y-axis label for `ax` to `'training loss/accuracy'`.\n","- `ax2 = ax.twinx()` creates a second y-axis (`ax2`) that shares the same x-axis as `ax`.\n","- `ax2.plot(total_loss_val, label='validation loss', color='orange')` plots the validation loss on the second y-axis (`ax2`). `total_loss_val` is a list of losses obtained during validation. The label `validation loss` is used to identify this line in the legend. The line color is set to orange using `color='orange'`.\n","- `ax2.plot(total_acc_val, label='validation accuracy', color='green')` plots the validation accuracy on the second y-axis (`ax2`). `total_acc_val` is a list of accuracies obtained during validation. The label `validation accuracy` is used to identify this line in the legend. The line color is set to green using `color='green'`.\n","- `ax2.set_ylabel('validation loss/accuracy')` sets the y-axis label for `ax2` to `'validation loss/accuracy'`.\n","- `ax.set_xlabel('epoch')` sets the x-axis label to `'epoch'`.\n","- `ax.set_title('Training and Validation Loss/Accuracy')` sets the plot title to `'Training and Validation Loss/Accuracy'`.\n","- `plt.legend()` displays the legend on the plot.\n","- `plt.tight_layout()` adjusts the spacing between the subplots to prevent overlapping.\n","- `plt.show()` displays the plot."],"metadata":{"id":"rJRyOW21dOUv"}},{"cell_type":"code","source":[],"metadata":{"id":"pHVDWHY-dOUv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fig, ax = plt.subplots(figsize=(8, 6))\n","\n","ax.plot(total_loss_train, label='training loss')\n","ax.plot(total_acc_train, label='training accuracy')\n","ax.set_ylabel('training loss/accuracy')\n","ax2 = ax.twinx()\n","ax2.plot(total_loss_val, label='validation loss', color='orange')\n","ax2.plot(total_acc_val, label='validation accuracy', color='green')\n","ax2.set_ylabel('validation loss/accuracy')\n","ax.set_xlabel('epoch')\n","ax.set_title('Training and Validation Loss/Accuracy')\n","plt.legend()\n","plt.tight_layout()\n","plt.show()"],"metadata":{"execution":{"iopub.status.busy":"2023-05-01T18:25:06.964262Z","iopub.status.idle":"2023-05-01T18:25:06.964812Z"},"trusted":true,"id":"Z908WjhTdOUw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This is a Python function that takes a confusion matrix and some additional parameters as input, and generates a plot of the confusion matrix using the Matplotlib library. The parameters are:\n","\n","- `cm`: the confusion matrix, represented as a 2D NumPy array.\n","- `classes`: a list of class names. The names should correspond to the rows and columns of the confusion matrix.\n","- `normalize`: a boolean indicating whether to normalize the confusion matrix. If set to `True`, each row of the confusion matrix will be divided by the total number of samples in that class. This can be useful when the classes are imbalanced.\n","- `title`: a string that sets the title of the plot.\n","- `cmap`: a colormap used to display the matrix.\n","\n","The function first displays the confusion matrix using `plt.imshow()`, which generates a heatmap with colors indicating the values in the matrix. The `title` parameter is used to set the title of the plot, and `plt.colorbar()` adds a colorbar to the plot to show the values associated with each color.\n","\n","The x and y axes of the plot are labeled with the `classes` list using `plt.xticks()` and `plt.yticks()`. If `normalize` is `True`, the confusion matrix is normalized using `cm.sum(axis=1)[:, np.newaxis]` to compute the fraction of correct predictions in each class. The `thresh` variable is used to set a threshold value for text color based on the maximum value in the confusion matrix.\n","\n","Finally, the function sets the x and y axis labels using `plt.xlabel()` and `plt.ylabel()`, respectively, and uses `plt.tight_layout()` to adjust the spacing between the subplots to prevent overlapping."],"metadata":{"id":"546EKyKsdOUw"}},{"cell_type":"code","source":["def plot_confusion_matrix(cm, classes,\n","                          normalize=False,\n","                          title='Confusion matrix',\n","                          cmap=plt.cm.Blues):\n","    \"\"\"\n","    This function prints and plots the confusion matrix.\n","    Normalization can be applied by setting `normalize=True`.\n","    \"\"\"\n","    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n","    plt.title(title)\n","    plt.colorbar()\n","    tick_marks = np.arange(len(classes))\n","    plt.xticks(tick_marks, classes, rotation=45)\n","    plt.yticks(tick_marks, classes)\n","\n","    if normalize:\n","        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n","\n","    thresh = cm.max() / 2.\n","    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n","        plt.text(j, i, cm[i, j],\n","                 horizontalalignment=\"center\",\n","                 color=\"white\" if cm[i, j] > thresh else \"black\")\n","\n","    plt.tight_layout()\n","    plt.ylabel('True label')\n","    plt.xlabel('Predicted label')"],"metadata":{"execution":{"iopub.status.busy":"2023-05-01T18:25:06.965630Z","iopub.status.idle":"2023-05-01T18:25:06.966056Z"},"trusted":true,"id":"Kjbb2utYdOUw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This code segment evaluates the performance of a PyTorch model on a validation dataset and generates a confusion matrix plot using the `confusion_matrix` and `plot_confusion_matrix` functions defined earlier.\n","\n","- `model.eval()` sets the model to evaluation mode.\n","- `y_label = []` initializes an empty list to store the true labels.\n","- `y_predict = []` initializes an empty list to store the predicted labels.\n","- `with torch.no_grad():` disables gradient calculation during evaluation for efficiency.\n","- `for i, data in enumerate(val_loader):` iterates over the validation data loader.\n","- `images, labels = data` unpacks the inputs and labels from the current batch.\n","- `N = images.size(0)` computes the batch size.\n","- `images = Variable(images).to(device)` converts the inputs to PyTorch variables and moves them to the specified device (e.g., CPU or GPU).\n","- `outputs = model(images)` feeds the inputs through the model to obtain the predicted outputs.\n","- `prediction = outputs.max(1, keepdim=True)[1]` obtains the predicted labels by selecting the class with the highest probability.\n","- `y_label.extend(labels.cpu().numpy())` appends the true labels to the `y_label` list.\n","- `y_predict.extend(np.squeeze(prediction.cpu().numpy().T))` appends the predicted labels to the `y_predict` list.\n","\n","After the loop, the confusion matrix is computed using the `confusion_matrix` function on `y_label` and `y_predict`.\n","\n","`plot_labels = ['akiec', 'bcc', 'bkl', 'df', 'nv', 'vasc', 'mel']` defines the label names for the confusion matrix plot.\n","\n","Finally, the `plot_confusion_matrix` function is called to generate the plot using the `confusion_mtx` and `plot_labels` as inputs."],"metadata":{"id":"j3_10Z9ndOUw"}},{"cell_type":"code","source":["model.eval()\n","y_label = []\n","y_predict = []\n","with torch.no_grad():\n","    for i, data in enumerate(val_loader):\n","        images, labels = data\n","        N = images.size(0)\n","        images = Variable(images).to(device)\n","        outputs = model(images)\n","        prediction = outputs.max(1, keepdim=True)[1]\n","        y_label.extend(labels.cpu().numpy())\n","        y_predict.extend(np.squeeze(prediction.cpu().numpy().T))\n","\n","# compute the confusion matrix\n","confusion_mtx = confusion_matrix(y_label, y_predict)\n","# plot the confusion matrix\n","plot_labels = ['akiec', 'bcc', 'bkl', 'df', 'nv', 'vasc','mel']\n","plot_confusion_matrix(confusion_mtx, plot_labels)"],"metadata":{"execution":{"iopub.status.busy":"2023-05-01T18:25:06.966722Z","iopub.status.idle":"2023-05-01T18:25:06.967303Z"},"trusted":true,"id":"ZsD-uINzdOUw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["`classification_report` is a function from scikit-learn library that generates a classification report containing various evaluation metrics such as precision, recall, F1-score, and support for each class.\n","\n","`y_label` and `y_predict` are the true and predicted labels, respectively.\n","\n","`target_names=plot_labels` specifies the label names for each class.\n","\n","The `classification_report` function returns a formatted string that contains the evaluation metrics for each class and their weighted average. This string is printed using the `print` function."],"metadata":{"id":"excfFLxPdOUx"}},{"cell_type":"code","source":["# Generate a classification report\n","report = classification_report(y_label, y_predict, target_names=plot_labels)\n","print(report)"],"metadata":{"execution":{"iopub.status.busy":"2023-05-01T18:25:06.968087Z","iopub.status.idle":"2023-05-01T18:25:06.971648Z"},"trusted":true,"id":"dJn-j15ndOUx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This code computes the fraction of samples that are misclassified for each true label and generates a bar plot of the results.\n","\n","`label_frac_error = 1 - np.diag(confusion_mtx) / np.sum(confusion_mtx, axis=1)` calculates the fraction of misclassified samples for each true label by subtracting the diagonal values of the confusion matrix (i.e., the correctly classified samples) from 1 and dividing by the total number of samples for each label.\n","\n","`plt.bar(np.arange(7), label_frac_error)` creates a bar plot with 7 bars (one for each label) with the `x` axis representing the label index and the `y` axis representing the fraction of samples classified incorrectly. \n","\n","`plt.xlabel('True Label')` sets the label for the `x` axis to \"True Label\".\n","\n","`plt.ylabel('Fraction classified incorrectly')` sets the label for the `y` axis to \"Fraction classified incorrectly\"."],"metadata":{"id":"X4PxMHYBdOUx"}},{"cell_type":"code","source":["label_frac_error = 1 - np.diag(confusion_mtx) / np.sum(confusion_mtx, axis=1)\n","plt.bar(np.arange(7),label_frac_error)\n","plt.xlabel('True Label')\n","plt.ylabel('Fraction classified incorrectly')"],"metadata":{"execution":{"iopub.status.busy":"2023-05-01T18:25:06.972549Z","iopub.status.idle":"2023-05-01T18:25:06.973278Z"},"trusted":true,"id":"4UpyL0lIdOUx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"Mrx26WwwdOUx"}},{"cell_type":"markdown","source":["This code saves the state dictionary of the PyTorch model to a file named `model.pth`. The `state_dict()` method returns a dictionary containing the parameters and persistent buffers of the model. The `torch.save()` function saves the dictionary to a file in a binary format that can be loaded later using the `torch.load()` function. By saving the model, we can load it later for further training or evaluation without having to retrain it from scratch."],"metadata":{"id":"_IYKWh_xdOUx"}},{"cell_type":"code","source":["# save model to file\n","torch.save(model.state_dict(), 'model.pth')\n"],"metadata":{"execution":{"iopub.status.busy":"2023-05-01T18:25:06.974117Z","iopub.status.idle":"2023-05-01T18:25:06.974864Z"},"trusted":true,"id":"ig98mthAdOUx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#To load the model from the file, you can use the torch.load() function as follows:\n","# load the saved model\n","#model = MyModelClass()\n","#model.load_state_dict(torch.load('model.pth'))"],"metadata":{"execution":{"iopub.status.busy":"2023-05-01T18:25:06.975728Z","iopub.status.idle":"2023-05-01T18:25:06.976462Z"},"trusted":true,"id":"R4v_XaFddOUy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["!pip install google-colab"],"metadata":{"id":"uS59ZLKydOUy"}},{"cell_type":"markdown","source":["!pip install google-colab"],"metadata":{"execution":{"iopub.status.busy":"2023-03-26T15:06:36.889664Z","iopub.status.idle":"2023-03-26T15:06:36.890416Z"},"id":"OVD84xV5dOUy"}},{"cell_type":"code","source":[],"metadata":{"id":"N7mlclNodOUy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["n = 6\n","from IPython import get_ipython\n","ipython = get_ipython()\n","\n","filename = f'model_{n}.pth'\n","torch.save(model.state_dict(), filename)\n","\n","# download model file\n","from IPython.display import FileLink\n","\n","remote_url = FileLink(f'model_{n}.pth')\n","\n","# download the file to your local machine\n","from google.colab import files\n","files.download('/kaggle/working/model.pth')"],"metadata":{"execution":{"iopub.status.busy":"2023-03-26T15:06:36.891386Z","iopub.status.idle":"2023-03-26T15:06:36.892151Z"},"id":"txQcxujbdOUy"}},{"cell_type":"markdown","source":[],"metadata":{"id":"YRLOUN2RdOUy"}},{"cell_type":"markdown","source":["## Conclusion"],"metadata":{"id":"8GITuNXOdOUy"}},{"cell_type":"markdown","source":["I tried to train with different network structures. When using Densenet-121, the average accuracy of 7 classes on the validation set can reach 92% in 10 epochs. We also calculated the confusion matrix for all classes and the F1-score for each class, which is a more comprehensive indicator that can take into account both the precision and recall of the classification model.Our model can achieve more than 90% on the F1-score indicator.\n","\n","Due to limited time, we did not spend much time on model training. By increasing in training epochs, adjustmenting of model hyperparameters, and attempting at different networks may further enhance the performance of the model."],"metadata":{"id":"BL5vAk6hdOUz"}},{"cell_type":"markdown","source":["## Next plan"],"metadata":{"id":"cL400ma2dOUz"}},{"cell_type":"markdown","source":["How to use image data and patient case data at the same time, my plan is to use CNN to extract features from images, use xgboost to convert medical records into vectors and then concat them with CNN network full-layer features. Two branch networks are trained simultaneously using a loss function. We can refer to the methods used in the advertising CTR estimation task."],"metadata":{"id":"vLki7QdUdOUz"}}]}